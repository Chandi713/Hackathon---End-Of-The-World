{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO, BytesIO\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT = os.path.join(os.getcwd(), 'Output')\n",
    "os.makedirs(OUTPUT, exist_ok=True)\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "    'Referer': 'https://www.fao.org/faostat/en/',\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# PIPELINE 1: WHO Disease Outbreak News\n",
    "# ============================================================\n",
    "\n",
    "def pipeline_who():\n",
    "    \"\"\"Fetch WHO DON data, extract outbreak features, save raw + features.\"\"\"\n",
    "    \n",
    "    url = 'https://www.who.int/api/news/diseaseoutbreaknews'\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (research-project)'}\n",
    "    \n",
    "    # --- Paginated fetch ---\n",
    "    all_items = []\n",
    "    skip, batch, limit = 0, 100, 5000\n",
    "    \n",
    "    while skip < limit:\n",
    "        try:\n",
    "            r = requests.get(url, headers=headers, timeout=15,)\n",
    "            r.raise_for_status()\n",
    "            items = r.json().get('value', [])\n",
    "        except:\n",
    "            break\n",
    "        if not items:\n",
    "            break\n",
    "        all_items.extend(items)\n",
    "        if len(items) < batch:\n",
    "            break\n",
    "        skip += batch\n",
    "    \n",
    "    if not all_items:\n",
    "        return None, None\n",
    "    \n",
    "    # --- Build raw DataFrame ---\n",
    "    df = pd.DataFrame(all_items)\n",
    "    df['PublicationDate'] = pd.to_datetime(df['PublicationDate'], errors='coerce')\n",
    "    df = df.sort_values('PublicationDate', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    def clean_html(html):\n",
    "        if pd.isna(html) or not html:\n",
    "            return ''\n",
    "        return BeautifulSoup(str(html), 'html.parser').get_text(' ', strip=True)\n",
    "    \n",
    "    for col in ['Summary', 'Epidemiology', 'Assessment', 'Overview', 'Response', 'Advice']:\n",
    "        if col in df.columns:\n",
    "            df[col + '_clean'] = df[col].apply(clean_html)\n",
    "    \n",
    "    if 'ItemDefaultUrl' in df.columns:\n",
    "        df['URL'] = df['ItemDefaultUrl'].apply(\n",
    "            lambda x: f'https://www.who.int{x}' if x and not str(x).startswith('http') else str(x))\n",
    "    \n",
    "    # --- Extract structured features ---\n",
    "    def extract(row):\n",
    "        title = str(row.get('Title', ''))\n",
    "        summary = str(row.get('Summary_clean', ''))\n",
    "        epi = str(row.get('Epidemiology_clean', ''))\n",
    "        assessment = str(row.get('Assessment_clean', ''))\n",
    "        response = str(row.get('Response_clean', ''))\n",
    "        advice = str(row.get('Advice_clean', ''))\n",
    "        combined = f'{summary} {epi}'\n",
    "        \n",
    "        parts = re.split(r'\\s*[-\\u2013\\u2014]\\s*', title, maxsplit=1)\n",
    "        disease = parts[0].strip() if parts else ''\n",
    "        country = parts[-1].strip() if len(parts) > 1 else ''\n",
    "        \n",
    "        case_nums = re.findall(r'(\\d[\\d,]*)\\s*(?:confirmed\\s+)?cases', combined, re.I)\n",
    "        cases = [int(n.replace(',', '')) for n in case_nums if n.strip()]\n",
    "        \n",
    "        death_nums = re.findall(r'(\\d[\\d,]*)\\s*deaths?', combined, re.I)\n",
    "        deaths = [int(n.replace(',', '')) for n in death_nums if n.strip()]\n",
    "        \n",
    "        cfr_m = re.search(r'(?:fatality|CFR)[^\\d]*(\\d+\\.?\\d*)%', combined, re.I)\n",
    "        \n",
    "        risk = 'unknown'\n",
    "        al = assessment.lower()\n",
    "        if 'very high' in al: risk = 'very_high'\n",
    "        elif 'high' in al and 'not high' not in al: risk = 'high'\n",
    "        elif 'moderate' in al: risk = 'moderate'\n",
    "        elif 'low' in al: risk = 'low'\n",
    "        \n",
    "        dl = disease.lower()\n",
    "        dtype = 'other'\n",
    "        if any(k in dl for k in ['cholera','typhoid']): dtype = 'waterborne'\n",
    "        elif any(k in dl for k in ['influenza','mers','covid','sars']): dtype = 'respiratory'\n",
    "        elif any(k in dl for k in ['ebola','marburg','lassa']): dtype = 'hemorrhagic_fever'\n",
    "        elif any(k in dl for k in ['dengue','zika','malaria']): dtype = 'vector_borne'\n",
    "        elif any(k in dl for k in ['measles','polio']): dtype = 'vaccine_preventable'\n",
    "        elif any(k in dl for k in ['plague','mpox']): dtype = 'zoonotic'\n",
    "        \n",
    "        return {\n",
    "            'date': row.get('PublicationDate'),\n",
    "            'don_id': row.get('DonId', ''),\n",
    "            'disease': disease, 'disease_type': dtype, 'country': country,\n",
    "            'total_cases': max(cases) if cases else None,\n",
    "            'total_deaths': max(deaths) if deaths else None,\n",
    "            'cfr_pct': float(cfr_m.group(1)) if cfr_m else None,\n",
    "            'who_risk_level': risk,\n",
    "            'multi_country': bool(re.search(r'international|multiple countries|global', combined, re.I)),\n",
    "            'intl_response': bool(re.search(r'WHO|deployed|GOARN', response, re.I)),\n",
    "            'travel_advisory': bool(re.search(r'travel|restriction|border', advice, re.I)),\n",
    "            'vaccination': bool(re.search(r'vaccin', f'{response} {advice}', re.I)),\n",
    "            'epi_length': len(epi), 'summary_length': len(summary),\n",
    "        }\n",
    "    \n",
    "    feat = df.apply(extract, axis=1, result_type='expand')\n",
    "    feat['date'] = pd.to_datetime(feat['date'], errors='coerce')\n",
    "    feat = feat.sort_values('date', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return df, feat\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PIPELINE 2: FAOSTAT Crop & Livestock Production\n",
    "# ============================================================\n",
    "\n",
    "def pipeline_faostat_production():\n",
    "    \"\"\"Fetch FAOSTAT crop/livestock production data.\"\"\"\n",
    "    \n",
    "    # Strategy 1: SDMX API\n",
    "    urls = [\n",
    "        'https://nsi-release-ro-statsuite.fao.org/rest/data/FAO,DF_CROP_LS_PROD,1.0/A..5412.0111..?startPeriod=2018&endPeriod=2023&format=csvfilewithlabels',\n",
    "        'https://nsi-release-ro-statsuite.fao.org/rest/data/FAO,DF_CROP_LS_PROD,1.0/all?startPeriod=2022&endPeriod=2023&format=csvfilewithlabels',\n",
    "    ]\n",
    "    for url in urls:\n",
    "        try:\n",
    "            r = requests.get(url, timeout=120)\n",
    "            r.raise_for_status()\n",
    "            return pd.read_csv(StringIO(r.text))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Strategy 2: faostat package\n",
    "    try:\n",
    "        import faostat\n",
    "        return faostat.get_data_df('QCL', pars={'element': [5510]})\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Strategy 3: Bulk ZIP\n",
    "    bulk = [\n",
    "        'https://bulks-faostat.fao.org/production/Production_Crops_Livestock_E_All_Data_(Normalized).csv.zip',\n",
    "        'https://fenixservices.fao.org/faostat/static/bulkdownloads/Production_Crops_Livestock_E_All_Data_(Normalized).zip',\n",
    "    ]\n",
    "    for url in bulk:\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS, timeout=120, allow_redirects=True)\n",
    "            r.raise_for_status()\n",
    "            z = zipfile.ZipFile(BytesIO(r.content))\n",
    "            csv_name = [f for f in z.namelist() if f.endswith('.csv')][0]\n",
    "            return pd.read_csv(z.open(csv_name), encoding='latin-1', nrows=100000)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Strategy 4: OWID mirror\n",
    "    for url in ['https://catalog.ourworldindata.org/garden/faostat/2024-03-14/faostat_qcl/faostat_qcl.csv']:\n",
    "        try:\n",
    "            return pd.read_csv(url, nrows=100000)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PIPELINE 3: FAOSTAT Trade (Exports + Imports)\n",
    "# ============================================================\n",
    "\n",
    "def pipeline_faostat_trade(trade_type='both'):\n",
    "    \"\"\"Fetch FAOSTAT trade data. trade_type: 'exports', 'imports', or 'both'.\"\"\"\n",
    "    \n",
    "    elem_map = {'exports': '5910+5922', 'imports': '5610+5622', 'both': '5910+5922+5610+5622'}\n",
    "    elems = elem_map.get(trade_type, elem_map['both'])\n",
    "    \n",
    "    # Strategy 1: SDMX API\n",
    "    for tf in ['DF_TRADE_CL', 'DF_TRADE', 'DF_TCL']:\n",
    "        try:\n",
    "            url = (f'https://nsi-release-ro-statsuite.fao.org/rest/data/'\n",
    "                   f'FAO,{tf},1.0/A..{elems}..?'\n",
    "                   f'startPeriod=2018&endPeriod=2023&format=csvfilewithlabels')\n",
    "            r = requests.get(url, timeout=90)\n",
    "            if r.status_code == 200 and len(r.text) > 500:\n",
    "                return pd.read_csv(StringIO(r.text), nrows=300000)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Strategy 2: Bulk ZIP\n",
    "    bulk = [\n",
    "        'https://bulks-faostat.fao.org/production/Trade_CropsLivestock_E_All_Data_(Normalized).csv.zip',\n",
    "        'https://fenixservices.fao.org/faostat/static/bulkdownloads/Trade_CropsLivestock_E_All_Data_(Normalized).zip',\n",
    "    ]\n",
    "    for url in bulk:\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS, timeout=120, allow_redirects=True)\n",
    "            if r.status_code == 200 and len(r.content) > 1000:\n",
    "                z = zipfile.ZipFile(BytesIO(r.content))\n",
    "                csv_name = [f for f in z.namelist() if f.endswith('.csv')][0]\n",
    "                df = pd.read_csv(z.open(csv_name), encoding='latin-1', nrows=500000)\n",
    "                if 'Element' in df.columns and trade_type != 'both':\n",
    "                    el = df['Element'].str.lower()\n",
    "                    df = df[el.str.contains(trade_type.rstrip('s'), na=False)]\n",
    "                return df\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Strategy 3: Detailed Trade Matrix\n",
    "    try:\n",
    "        url = 'https://fenixservices.fao.org/faostat/static/bulkdownloads/Trade_DetailedTradeMatrix_E_All_Data_(Normalized).zip'\n",
    "        r = requests.get(url, headers=HEADERS, timeout=300, stream=True)\n",
    "        if r.status_code == 200:\n",
    "            z = zipfile.ZipFile(BytesIO(r.content))\n",
    "            csv_name = [f for f in z.namelist() if f.endswith('.csv')][0]\n",
    "            df = pd.read_csv(z.open(csv_name), encoding='latin-1', nrows=300000)\n",
    "            if 'Element' in df.columns and trade_type != 'both':\n",
    "                el = df['Element'].str.lower()\n",
    "                df = df[el.str.contains(trade_type.rstrip('s'), na=False)]\n",
    "            return df\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Strategy 4: faostat package\n",
    "    try:\n",
    "        import faostat\n",
    "        pars = {}\n",
    "        if trade_type == 'exports': pars = {'element': [5910, 5922]}\n",
    "        elif trade_type == 'imports': pars = {'element': [5610, 5622]}\n",
    "        return faostat.get_data_df('TCL', pars=pars)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SAVE HELPER\n",
    "# ============================================================\n",
    "\n",
    "def save(df, filename):\n",
    "    \"\"\"Save DataFrame to Output folder as CSV.\"\"\"\n",
    "    if df is None or (isinstance(df, pd.DataFrame) and df.empty):\n",
    "        print(f'  [FAIL] {filename}: no data to save')\n",
    "        return False\n",
    "    path = os.path.join(OUTPUT, filename)\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f'  [SAVED] {filename} ({df.shape[0]:,} rows x {df.shape[1]} cols) -> \\\"{OUTPUT}\\\"')\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folder: /Users/smitchandi/Documents/Coding/Practice/Hackathon/Output\n",
      "Run started: 2026-02-14 20:07:19\n",
      "============================================================\n",
      "\n",
      "[Pipeline-1] WHO Disease Outbreak News — processing...\n",
      "  [SAVED] who_don_raw.csv (50 rows x 29 cols) -> \"/Users/smitchandi/Documents/Coding/Practice/Hackathon/Output\"\n",
      "  [SAVED] who_don_features.csv (50 rows x 15 cols) -> \"/Users/smitchandi/Documents/Coding/Practice/Hackathon/Output\"\n",
      "[Pipeline-1] WHO Disease Outbreak News — SUCCESS\n",
      "\n",
      "[Pipeline-2] FAOSTAT Production — processing...\n",
      "Warning: seems like no data are available for your selection.\n",
      "  [SAVED] faostat_production.csv (100,000 rows x 14 cols) -> \"/Users/smitchandi/Documents/Coding/Practice/Hackathon/Output\"\n",
      "[Pipeline-2] FAOSTAT Production — SUCCESS\n",
      "\n",
      "[Pipeline-3] FAOSTAT Trade (Exports) — processing...\n",
      "  [SAVED] faostat_trade_exports.csv (188,329 rows x 14 cols) -> \"/Users/smitchandi/Documents/Coding/Practice/Hackathon/Output\"\n",
      "[Pipeline-3] FAOSTAT Trade (Exports) — SUCCESS\n",
      "\n",
      "[Pipeline-4] FAOSTAT Trade (Imports) — processing...\n",
      "  [SAVED] faostat_trade_imports.csv (311,671 rows x 14 cols) -> \"/Users/smitchandi/Documents/Coding/Practice/Hackathon/Output\"\n",
      "[Pipeline-4] FAOSTAT Trade (Imports) — SUCCESS\n",
      "\n",
      "============================================================\n",
      "PIPELINE SUMMARY\n",
      "============================================================\n",
      "  [OK] WHO_DON: SUCCESS\n",
      "  [OK] FAOSTAT_Production: SUCCESS\n",
      "  [OK] FAOSTAT_Exports: SUCCESS\n",
      "  [OK] FAOSTAT_Imports: SUCCESS\n",
      "\n",
      "Files in Output:\n",
      "  faostat_production.csv                     9144.6 KB\n",
      "  faostat_trade_exports.csv                 23838.7 KB\n",
      "  faostat_trade_imports.csv                 38887.0 KB\n",
      "  who_don_features.csv                          6.4 KB\n",
      "  who_don_raw.csv                             401.1 KB\n",
      "\n",
      "All data fetched successfully and saved in \"/Users/smitchandi/Documents/Coding/Practice/Hackathon/Output\"\n",
      "Run finished: 2026-02-14 20:08:14\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# RUN PIPELINE\n",
    "# ============================================================\n",
    "\n",
    "print(f'Output folder: {OUTPUT}')\n",
    "print(f'Run started: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "print('=' * 60)\n",
    "\n",
    "results = {}\n",
    "\n",
    "# --- Pipeline 1: WHO DON ---\n",
    "print('\\n[Pipeline-1] WHO Disease Outbreak News — processing...')\n",
    "try:\n",
    "    who_raw, who_features = pipeline_who()\n",
    "    s1 = save(who_raw, 'who_don_raw.csv')\n",
    "    s2 = save(who_features, 'who_don_features.csv')\n",
    "    results['WHO_DON'] = 'SUCCESS' if (s1 or s2) else 'FAIL'\n",
    "except Exception as e:\n",
    "    results['WHO_DON'] = f'FAIL: {e}'\n",
    "print(f'[Pipeline-1] WHO Disease Outbreak News — {results[\"WHO_DON\"]}')\n",
    "\n",
    "# --- Pipeline 2: FAOSTAT Production ---\n",
    "print('\\n[Pipeline-2] FAOSTAT Production — processing...')\n",
    "try:\n",
    "    df_prod = pipeline_faostat_production()\n",
    "    s = save(df_prod, 'faostat_production.csv')\n",
    "    results['FAOSTAT_Production'] = 'SUCCESS' if s else 'FAIL'\n",
    "except Exception as e:\n",
    "    results['FAOSTAT_Production'] = f'FAIL: {e}'\n",
    "print(f'[Pipeline-2] FAOSTAT Production — {results[\"FAOSTAT_Production\"]}')\n",
    "\n",
    "# --- Pipeline 3: FAOSTAT Trade ---\n",
    "print('\\n[Pipeline-3] FAOSTAT Trade (Exports) — processing...')\n",
    "try:\n",
    "    df_exp = pipeline_faostat_trade('exports')\n",
    "    s = save(df_exp, 'faostat_trade_exports.csv')\n",
    "    results['FAOSTAT_Exports'] = 'SUCCESS' if s else 'FAIL'\n",
    "except Exception as e:\n",
    "    results['FAOSTAT_Exports'] = f'FAIL: {e}'\n",
    "print(f'[Pipeline-3] FAOSTAT Trade (Exports) — {results[\"FAOSTAT_Exports\"]}')\n",
    "\n",
    "print('\\n[Pipeline-4] FAOSTAT Trade (Imports) — processing...')\n",
    "try:\n",
    "    df_imp = pipeline_faostat_trade('imports')\n",
    "    s = save(df_imp, 'faostat_trade_imports.csv')\n",
    "    results['FAOSTAT_Imports'] = 'SUCCESS' if s else 'FAIL'\n",
    "except Exception as e:\n",
    "    results['FAOSTAT_Imports'] = f'FAIL: {e}'\n",
    "print(f'[Pipeline-4] FAOSTAT Trade (Imports) — {results[\"FAOSTAT_Imports\"]}')\n",
    "\n",
    "# --- Summary ---\n",
    "print('\\n' + '=' * 60)\n",
    "print('PIPELINE SUMMARY')\n",
    "print('=' * 60)\n",
    "all_ok = True\n",
    "for name, status in results.items():\n",
    "    icon = 'OK' if status == 'SUCCESS' else 'XX'\n",
    "    if status != 'SUCCESS': all_ok = False\n",
    "    print(f'  [{icon}] {name}: {status}')\n",
    "\n",
    "print('\\nFiles in Output:')\n",
    "for f in sorted(os.listdir(OUTPUT)):\n",
    "    size = os.path.getsize(os.path.join(OUTPUT, f))\n",
    "    print(f'  {f:40s} {size/1024:>8.1f} KB')\n",
    "\n",
    "if all_ok:\n",
    "    print(f'\\nAll data fetched successfully and saved in \\\"{OUTPUT}\\\"')\n",
    "else:\n",
    "    failed = [k for k, v in results.items() if v != 'SUCCESS']\n",
    "    print(f'\\nPartial success. Failed: {failed}')\n",
    "\n",
    "print(f'Run finished: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
