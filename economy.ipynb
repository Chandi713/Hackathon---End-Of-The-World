import os
import sys
import time
import pandas as pd
import requests
from datetime import datetime


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# GLOBAL CONFIGURATION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

BASE_DATA_DIR = "data"
START_YEAR = 2000
END_YEAR = 2025

# UN Comtrade API Key ‚Äî paste yours here
COMTRADE_API_KEY = "ddd147f9a63d4823ad41476c7c2e7d14"

# UN Comtrade years (limited for hackathon speed; expand if needed)
COMTRADE_YEARS = [2018, 2019, 2020, 2021, 2022, 2023, 2024]
# Full range (slower): COMTRADE_YEARS = list(range(2000, 2025))


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# PIPELINE 1: WORLD BANK API ‚Äî Economic Indicators
# Output: data/worldbank_api/
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

WB_OUTPUT_DIR = os.path.join(BASE_DATA_DIR, "worldbank_api")

WB_INDICATORS = {
    "NE.TRD.GNFS.ZS":   "trade_pct_gdp",
    "BN.CAB.XOKA.CD":   "current_account_balance",
    "PA.NUS.FCRF":       "exchange_rate_lcu_per_usd",
    "PX.REX.REER":       "real_effective_exchange_rate",
    "FP.CPI.TOTL.ZG":   "inflation_cpi_annual_pct",
    "FP.CPI.TOTL":       "cpi_index",
    "NY.GDP.PCAP.PP.CD": "gdp_per_capita_ppp",
    "NY.GDP.MKTP.CD":    "gdp_total_nominal",
}


def run_worldbank_api():
    """Fetch 8 economic indicators from World Bank API for all countries."""

    print("\n" + "=" * 70)
    print("PIPELINE 1: WORLD BANK API ‚Äî Economic Indicators")
    print(f"Indicators: {len(WB_INDICATORS)} | Years: {START_YEAR}-{END_YEAR}")
    print("=" * 70)

    try:
        import wbgapi as wb
    except ImportError:
        print("‚ùå Please install: pip install wbgapi")
        return False

    os.makedirs(WB_OUTPUT_DIR, exist_ok=True)
    time_range = range(START_YEAR, END_YEAR + 1)
    all_data = {}
    failed = []

    for indicator_code, friendly_name in WB_INDICATORS.items():
        print(f"\n   üìä Fetching: {friendly_name} ({indicator_code})...")
        try:
            df = wb.data.DataFrame(
                indicator_code,
                time=time_range,
                labels=True,
                columns="time",
                numericTimeKeys=True,
            )
            df = df.reset_index()
            if "economy" in df.columns:
                df = df.rename(columns={"economy": "country_code"})
            if "Country" in df.columns:
                df = df.rename(columns={"Country": "country_name"})

            filepath = os.path.join(WB_OUTPUT_DIR, f"{friendly_name}.csv")
            df.to_csv(filepath, index=False)
            print(f"      ‚úÖ {len(df)} countries, {df.shape[1]-2} years")
            all_data[friendly_name] = df

        except Exception as e:
            print(f"      ‚ùå Failed: {e}")
            failed.append((indicator_code, friendly_name, str(e)))

    # Create combined long-format dataset
    if all_data:
        print(f"\n   üîó Creating combined dataset...")
        combined_frames = []
        for friendly_name, df in all_data.items():
            id_cols = [c for c in df.columns if not str(c).isdigit()]
            year_cols = [c for c in df.columns if str(c).isdigit()]
            melted = df.melt(id_vars=id_cols, value_vars=year_cols,
                             var_name="year", value_name=friendly_name)
            melted["year"] = melted["year"].astype(int)
            combined_frames.append(melted)

        combined = combined_frames[0]
        merge_keys = ["country_code", "year"]
        if "country_name" in combined.columns:
            merge_keys.append("country_name")
        for frame in combined_frames[1:]:
            combined = combined.merge(frame, on=merge_keys, how="outer")

        combined = combined.sort_values(["country_code", "year"]).reset_index(drop=True)
        filepath = os.path.join(WB_OUTPUT_DIR, "economic_indicators_combined.csv")
        combined.to_csv(filepath, index=False)
        print(f"      ‚úÖ Combined: {combined.shape[0]} rows √ó {combined.shape[1]} cols")

    print(f"\n   ‚úÖ Pipeline 1 complete: {len(all_data)}/{len(WB_INDICATORS)} indicators fetched")
    if failed:
        print(f"   ‚ùå Failed: {[f[1] for f in failed]}")
    return True


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# PIPELINE 2: WORLD BANK PINK SHEET ‚Äî Commodity Prices
# Output: data/worldbank_pinksheet/
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

PS_OUTPUT_DIR = os.path.join(BASE_DATA_DIR, "worldbank_pinksheet")

PINK_SHEET_URLS = [
    "https://thedocs.worldbank.org/en/doc/18675f1d1639c7a34d463f59263ba0a2-0050012025/related/CMO-Historical-Data-Monthly.xlsx",
    "https://thedocs.worldbank.org/en/doc/561011486076393416-0050022017/related/CMO-Historical-Data-Monthly.xlsx",
]

PS_COMMODITIES = {
    "crude_oil_brent":    {"search_terms": ["crude oil, brent", "coal_brent", "brent"], "unit": "$/bbl", "chain": "fuel"},
    "crude_oil_wti":      {"search_terms": ["crude oil, wti", "coal_wti", "wti"], "unit": "$/bbl", "chain": "fuel"},
    "natural_gas_us":     {"search_terms": ["natural gas, us", "ngas_us", "natural gas, u.s"], "unit": "$/mmbtu", "chain": "fuel"},
    "natural_gas_europe": {"search_terms": ["natural gas, europe", "ngas_eur", "natural gas, eu"], "unit": "$/mmbtu", "chain": "fuel"},
    "coal_australia":     {"search_terms": ["coal, australia", "coal_aus"], "unit": "$/mt", "chain": "fuel"},
    "wheat":              {"search_terms": ["wheat, us hrw", "wheat_us", "wheat, us"], "unit": "$/mt", "chain": "food"},
    "rice":               {"search_terms": ["rice, thai", "rice_thai", "rice, thailand"], "unit": "$/mt", "chain": "food"},
    "maize_corn":         {"search_terms": ["maize", "corn"], "unit": "$/mt", "chain": "food"},
    "soybeans":           {"search_terms": ["soybeans", "soybean"], "unit": "$/mt", "chain": "food"},
    "sugar":              {"search_terms": ["sugar, world", "sugar_wrld", "sugar, free market"], "unit": "$/kg", "chain": "food"},
    "palm_oil":           {"search_terms": ["palm oil", "palmoil"], "unit": "$/mt", "chain": "food"},
    "fertilizer_dap":     {"search_terms": ["dap", "diammonium phosphate"], "unit": "$/mt", "chain": "food"},
    "fertilizer_urea":    {"search_terms": ["urea"], "unit": "$/mt", "chain": "food"},
    "phosphate_rock":     {"search_terms": ["phosphate rock", "phosphate"], "unit": "$/mt", "chain": "food"},
}


def _find_column(columns, search_terms):
    for col in columns:
        col_lower = str(col).lower().strip()
        for term in search_terms:
            if term.lower() in col_lower:
                return col
    return None


def run_pinksheet():
    """Download and process World Bank Pink Sheet commodity prices."""

    print("\n" + "=" * 70)
    print("PIPELINE 2: WORLD BANK PINK SHEET ‚Äî Commodity Prices")
    print(f"Commodities: {len(PS_COMMODITIES)} | From: {START_YEAR} to present")
    print("=" * 70)

    os.makedirs(PS_OUTPUT_DIR, exist_ok=True)

    # Step 1: Download
    local_path = os.path.join(PS_OUTPUT_DIR, "CMO-Historical-Data-Monthly.xlsx")
    if os.path.exists(local_path):
        print(f"\n   üìÅ File already exists: {local_path}")
    else:
        print(f"\n   ‚¨áÔ∏è  Downloading Pink Sheet...")
        downloaded = False
        for url in PINK_SHEET_URLS:
            try:
                resp = requests.get(url, timeout=60)
                resp.raise_for_status()
                with open(local_path, "wb") as f:
                    f.write(resp.content)
                print(f"      ‚úÖ Downloaded ({len(resp.content)/1024/1024:.1f} MB)")
                downloaded = True
                break
            except Exception as e:
                print(f"      ‚ö†Ô∏è Failed: {e}")
        if not downloaded:
            print("   ‚ùå Could not download Pink Sheet. Download manually from:")
            print("      https://www.worldbank.org/en/research/commodity-markets")
            return False

    # Step 2: Parse
    print(f"\n   üìÑ Parsing Excel file...")
    xl = pd.ExcelFile(local_path)
    sheet_name = None
    for name in ["Monthly Prices", "monthly_prices", "Sheet1"]:
        if name in xl.sheet_names:
            sheet_name = name
            break
    if sheet_name is None:
        sheet_name = xl.sheet_names[0]

    df_raw = pd.read_excel(local_path, sheet_name=sheet_name, header=None)

    # Find header row
    header_row = 0
    for i in range(min(10, len(df_raw))):
        row_str = " ".join([str(v).lower() for v in df_raw.iloc[i].values if pd.notna(v)])
        if any(term in row_str for term in ["crude oil", "natural gas", "wheat", "coal", "energy"]):
            header_row = i
            break

    df = pd.read_excel(local_path, sheet_name=sheet_name, header=header_row)
    date_col = df.columns[0]
    print(f"      Sheet: {sheet_name} | Header row: {header_row} | Columns: {len(df.columns)}")

    # Step 3: Extract commodities
    print(f"\n   üîç Extracting {len(PS_COMMODITIES)} commodities...")
    result = pd.DataFrame()
    dates = df[date_col].astype(str)
    parsed_dates = []
    for d in dates:
        d_clean = str(d).strip().replace("M", "-").replace("m", "-")
        try:
            parsed = pd.to_datetime(d_clean, format="%Y-%m", errors="coerce")
            if pd.isna(parsed):
                parsed = pd.to_datetime(d_clean, errors="coerce")
            parsed_dates.append(parsed)
        except:
            parsed_dates.append(pd.NaT)

    result["date"] = parsed_dates
    valid = result["date"].notna() & (result["date"].dt.year >= START_YEAR)
    result = result[valid].copy()
    df = df[valid].copy()
    result = result.reset_index(drop=True)
    df = df.reset_index(drop=True)
    result["year"] = result["date"].dt.year
    result["month"] = result["date"].dt.month

    matched = 0
    for friendly_name, config in PS_COMMODITIES.items():
        col = _find_column(list(df.columns), config["search_terms"])
        if col is not None:
            result[friendly_name] = pd.to_numeric(df[col], errors="coerce")
            matched += 1
            print(f"      ‚úÖ {friendly_name:25s} ‚Üê '{col}'")
        else:
            result[friendly_name] = None
            print(f"      ‚ùå {friendly_name:25s} ‚Üê no match")

    print(f"      Matched: {matched}/{len(PS_COMMODITIES)}")

    # Step 4: Derived metrics
    print(f"\n   üìê Computing derived metrics...")
    commodity_cols = [c for c in result.columns if c not in ["date", "year", "month"]]
    for col in commodity_cols:
        if result[col].notna().sum() > 12:
            result[f"{col}_mom_pct"] = result[col].pct_change() * 100
            result[f"{col}_yoy_pct"] = result[col].pct_change(12) * 100
            result[f"{col}_12m_avg"] = result[col].rolling(12).mean()

    # Step 5: Save outputs
    print(f"\n   üíæ Saving outputs...")

    # Full
    result.to_csv(os.path.join(PS_OUTPUT_DIR, "commodity_prices_full.csv"), index=False)

    # Clean
    base_cols = ["date", "year", "month"] + [
        c for c in result.columns if c not in ["date", "year", "month"]
        and not c.endswith(("_mom_pct", "_yoy_pct", "_12m_avg"))
    ]
    result[base_cols].to_csv(os.path.join(PS_OUTPUT_DIR, "commodity_prices_clean.csv"), index=False)

    # Annual
    annual = result.groupby("year").mean(numeric_only=True).reset_index()
    annual = annual.drop(columns=["month"], errors="ignore")
    annual_cols = ["year"] + [c for c in annual.columns if c != "year"
                              and not c.endswith(("_mom_pct", "_yoy_pct", "_12m_avg"))]
    annual[annual_cols].to_csv(os.path.join(PS_OUTPUT_DIR, "commodity_prices_annual.csv"), index=False)

    # Spikes
    spike_cols = [c for c in result.columns if c.endswith("_yoy_pct")]
    spike_records = []
    for _, row in result.iterrows():
        for col in spike_cols:
            if pd.notna(row[col]) and abs(row[col]) > 30:
                commodity = col.replace("_yoy_pct", "")
                spike_records.append({
                    "date": row["date"], "commodity": commodity,
                    "yoy_change_pct": round(row[col], 2),
                    "price": row.get(commodity),
                    "chain": PS_COMMODITIES.get(commodity, {}).get("chain", "unknown"),
                    "direction": "spike" if row[col] > 0 else "crash",
                })
    if spike_records:
        spikes_df = pd.DataFrame(spike_records).sort_values("date").reset_index(drop=True)
        spikes_df.to_csv(os.path.join(PS_OUTPUT_DIR, "commodity_price_spikes.csv"), index=False)
        print(f"      ‚úÖ {len(spikes_df)} spike/crash events detected")

    print(f"\n   ‚úÖ Pipeline 2 complete: {matched} commodities, {len(result)} months")
    return True


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# PIPELINE 3: UN COMTRADE ‚Äî Bilateral Trade Dependency
# Output: data/un_comtrade/
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

CT_OUTPUT_DIR = os.path.join(BASE_DATA_DIR, "un_comtrade")

CT_COMMODITY_HS_CODES = {
    "crude_oil":            {"hs_code": "2709", "description": "Petroleum oils, crude", "chain": "fuel"},
    "natural_gas":          {"hs_code": "2711", "description": "Petroleum gases (LNG, LPG)", "chain": "fuel"},
    "coal":                 {"hs_code": "2701", "description": "Coal", "chain": "fuel"},
    "wheat":                {"hs_code": "1001", "description": "Wheat and meslin", "chain": "food"},
    "rice":                 {"hs_code": "1006", "description": "Rice", "chain": "food"},
    "maize_corn":           {"hs_code": "1005", "description": "Maize (corn)", "chain": "food"},
    "soybeans":             {"hs_code": "1201", "description": "Soya beans", "chain": "food"},
    "sugar":                {"hs_code": "1701", "description": "Cane or beet sugar", "chain": "food"},
    "palm_oil":             {"hs_code": "1511", "description": "Palm oil", "chain": "food"},
    "fertilizer_mixed":     {"hs_code": "3105", "description": "Mineral fertilizers (DAP)", "chain": "food"},
    "fertilizer_nitrogen":  {"hs_code": "3102", "description": "Nitrogenous fertilizers (urea)", "chain": "food"},
    "phosphate_rock":       {"hs_code": "2510", "description": "Natural phosphates", "chain": "food"},
}


def _fetch_comtrade_flow(flow_code, flow_name, comtradeapicall):
    """Fetch import or export data for all commodities."""

    all_records = []
    for commodity_name, config in CT_COMMODITY_HS_CODES.items():
        hs_code = config["hs_code"]
        chain = config["chain"]

        for year in COMTRADE_YEARS:
            print(f"      üì¶ {flow_name.upper():7s} | {commodity_name:22s} | {year} | HS {hs_code}...")
            try:
                df = comtradeapicall.getFinalData(
                    COMTRADE_API_KEY,
                    typeCode="C", freqCode="A", clCode="HS",
                    period=str(year), reporterCode=None,
                    cmdCode=hs_code, flowCode=flow_code,
                    partnerCode=None, partner2Code=None,
                    customsCode=None, motCode=None,
                    maxRecords=100000, format_output="JSON",
                    aggregateBy=None, breakdownMode="classic",
                    countOnly=None, includeDesc=True,
                )
                if df is not None and len(df) > 0:
                    keep_cols = [
                        "refYear", "reporterCode", "reporterISO", "reporterDesc",
                        "partnerCode", "partnerISO", "partnerDesc",
                        "cmdCode", "cmdDesc", "flowCode", "flowDesc",
                        "primaryValue", "netWgt", "qty", "qtyUnitAbbr",
                    ]
                    available = [c for c in keep_cols if c in df.columns]
                    df_clean = df[available].copy()
                    df_clean["commodity_name"] = commodity_name
                    df_clean["chain"] = chain
                    all_records.append(df_clean)
                    print(f"         ‚úÖ {len(df_clean)} records")
                else:
                    print(f"         ‚ö†Ô∏è No data")
            except Exception as e:
                print(f"         ‚ùå {e}")
            time.sleep(1)

    if all_records:
        return pd.concat(all_records, ignore_index=True)
    return pd.DataFrame()


def _compute_dependency(df_imports):
    """Compute import dependency scores."""

    if df_imports.empty:
        return pd.DataFrame()

    records = []
    for (reporter, commodity, year), group in df_imports.groupby(
        ["reporterISO", "commodity_name", "refYear"]
    ):
        if "primaryValue" not in group.columns:
            continue
        total_value = group["primaryValue"].sum()
        if total_value <= 0:
            continue

        partner_shares = (
            group.groupby(["partnerISO", "partnerDesc"])["primaryValue"]
            .sum().sort_values(ascending=False)
        )
        top5 = partner_shares.head(5)
        shares = (partner_shares / total_value * 100)
        hhi = (shares ** 2).sum()

        top_iso = top5.index[0][0] if len(top5) > 0 else None
        top_name = top5.index[0][1] if len(top5) > 0 else None
        top_share = (top5.iloc[0] / total_value * 100) if len(top5) > 0 else 0

        records.append({
            "year": year, "importer": reporter, "commodity": commodity,
            "chain": group["chain"].iloc[0],
            "total_import_value_usd": total_value,
            "top_partner": top_iso, "top_partner_name": top_name,
            "top_partner_share_pct": round(top_share, 2),
            "top5_partners": ", ".join([f"{p[0]}({v/total_value*100:.0f}%)" for p, v in top5.items()]),
            "hhi_concentration": round(hhi, 2),
            "num_partners": len(partner_shares),
            "concentration_risk": "HIGH" if top_share > 50 else "MEDIUM" if top_share > 30 else "LOW",
        })

    return pd.DataFrame(records)


def run_comtrade():
    """Fetch bilateral trade data from UN Comtrade."""

    print("\n" + "=" * 70)
    print("PIPELINE 3: UN COMTRADE ‚Äî Bilateral Trade Dependency")
    print(f"Commodities: {len(CT_COMMODITY_HS_CODES)} | Years: {COMTRADE_YEARS}")
    print("=" * 70)

    if COMTRADE_API_KEY == "YOUR_COMTRADE_API_KEY_HERE":
        print("\n   ‚ö†Ô∏è No API key set! Skipping UN Comtrade.")
        print("   Register free at https://comtradeplus.un.org/")
        print("   Then set COMTRADE_API_KEY at the top of this script.")
        return False

    try:
        import comtradeapicall
    except ImportError:
        print("   ‚ùå Please install: pip install comtradeapicall")
        return False

    os.makedirs(CT_OUTPUT_DIR, exist_ok=True)

    # Fetch imports
    print(f"\n   üì• FETCHING IMPORTS...")
    df_imports = _fetch_comtrade_flow("M", "imports", comtradeapicall)

    # Fetch exports
    print(f"\n   üì§ FETCHING EXPORTS...")
    df_exports = _fetch_comtrade_flow("X", "exports", comtradeapicall)

    # Compute dependency
    print(f"\n   üìê Computing dependency scores...")
    df_dep = _compute_dependency(df_imports)

    # Save outputs
    print(f"\n   üíæ Saving outputs...")
    if not df_imports.empty:
        df_imports.to_csv(os.path.join(CT_OUTPUT_DIR, "bilateral_imports_raw.csv"), index=False)
        print(f"      ‚úÖ bilateral_imports_raw.csv ({len(df_imports)} records)")
    if not df_exports.empty:
        df_exports.to_csv(os.path.join(CT_OUTPUT_DIR, "bilateral_exports_raw.csv"), index=False)
        print(f"      ‚úÖ bilateral_exports_raw.csv ({len(df_exports)} records)")
    if not df_dep.empty:
        df_dep.to_csv(os.path.join(CT_OUTPUT_DIR, "import_dependency_scores.csv"), index=False)
        high_risk = df_dep[df_dep["concentration_risk"] == "HIGH"]
        if not high_risk.empty:
            high_risk.to_csv(os.path.join(CT_OUTPUT_DIR, "high_risk_dependencies.csv"), index=False)
            print(f"      ‚úÖ high_risk_dependencies.csv ({len(high_risk)} entries)")
        print(f"      ‚úÖ import_dependency_scores.csv ({len(df_dep)} entries)")
    if not df_exports.empty and "primaryValue" in df_exports.columns:
        export_summary = (
            df_exports.groupby(["refYear", "reporterISO", "reporterDesc", "commodity_name", "chain"])
            ["primaryValue"].sum().reset_index()
            .sort_values(["commodity_name", "refYear", "primaryValue"], ascending=[True, True, False])
        )
        export_summary.to_csv(os.path.join(CT_OUTPUT_DIR, "export_volumes_by_country.csv"), index=False)
        print(f"      ‚úÖ export_volumes_by_country.csv")

    print(f"\n   ‚úÖ Pipeline 3 complete: {len(df_imports)} imports, {len(df_exports)} exports")
    return True


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# MAIN ‚Äî RUN ALL PIPELINES
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def print_final_summary():
    """Print summary of all data files created."""

    print("\n\n" + "=" * 70)
    print("üìã FINAL DATA INVENTORY")
    print("=" * 70)

    total_files = 0
    total_size = 0

    for folder in ["worldbank_api", "worldbank_pinksheet", "un_comtrade"]:
        folder_path = os.path.join(BASE_DATA_DIR, folder)
        if not os.path.exists(folder_path):
            print(f"\n   üìÅ {folder}/ ‚Äî NOT CREATED")
            continue

        files = [f for f in os.listdir(folder_path) if f.endswith((".csv", ".xlsx"))]
        folder_size = sum(os.path.getsize(os.path.join(folder_path, f)) for f in files)
        print(f"\n   üìÅ {folder}/ ({len(files)} files, {folder_size/1024:.1f} KB)")
        for f in sorted(files):
            size = os.path.getsize(os.path.join(folder_path, f))
            print(f"      {f:50s} {size/1024:>8.1f} KB")
            total_files += 1
            total_size += size

    print(f"\n   {'‚îÄ' * 60}")
    print(f"   Total: {total_files} files, {total_size/1024/1024:.2f} MB")


def main():
    start_time = datetime.now()

    print("=" * 70)
    print("ResilienceAI ‚Äî Unified Economic Data Pipeline")
    print(f"Timestamp: {start_time.isoformat()}")
    print("=" * 70)

    # Parse CLI flags
    args = sys.argv[1:]
    run_all = len(args) == 0
    run_wb = run_all or "--worldbank" in args
    run_ps = run_all or "--pinksheet" in args
    run_ct = run_all or "--comtrade" in args

    os.makedirs(BASE_DATA_DIR, exist_ok=True)

    results = {}

    # Pipeline 1: World Bank API
    if run_wb:
        results["World Bank API"] = run_worldbank_api()

    # Pipeline 2: Pink Sheet
    if run_ps:
        results["Pink Sheet"] = run_pinksheet()

    # Pipeline 3: UN Comtrade
    if run_ct:
        results["UN Comtrade"] = run_comtrade()

    # Final summary
    print_final_summary()

    elapsed = (datetime.now() - start_time).total_seconds()
    print(f"\n‚è±Ô∏è  Total time: {elapsed:.1f} seconds")

    print(f"\n{'=' * 70}")
    print("PIPELINE STATUS")
    print("=" * 70)
    for name, status in results.items():
        icon = "‚úÖ" if status else "‚ùå"
        print(f"   {icon} {name}")

    print(f"\n‚úÖ Economic data pipeline complete!")
    print(f"üìÅ All data saved to: {os.path.abspath(BASE_DATA_DIR)}/")


if __name__ == "__main__":
    main()